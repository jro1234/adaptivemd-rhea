{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 1- `AdaptiveMD` basics\n",
    "\n",
    "#### First we cover some basics demonstrating the use of `AdaptiveMD` objects to get you going.\n",
    "\n",
    "We will briefly talk about\n",
    "\n",
    "1. resources\n",
    "2. files\n",
    "3. generators\n",
    "4. how to run a simple trajectory\n",
    "\n",
    "All of these objects from the `adaptivemd` package are used to organize a workflow by associating them with the `adaptivemd.Project` class. We will create a new project called \"*tutorial*\" that is used in this and subsequent tutorial notebooks, so as with any project, be careful about deleting work if revisiting this notebook. All python packages specified in the README must be installed on your local machine, along with MongoDB. The first tutorials can easily be modified to test the use of other resources, so make any necessary adjustments. The `mongod` process must be running somewhere visible to this notebook session and the defined `resource`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Project`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's load the package and import the `Project` class since we want to start a project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from adaptivemd import Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open a project with a UNIQUE name by instantiating `Project` with a name. This will be the name used in the DB so make sure it is new and not too short. Calling `adaptivemd.Project` with a name to construct an instance will always create a non-existing project, or reopen an exising one. You cannot chose between opening types as you would with a file. This is a precaution to not accidentally delete your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see what projects exist already by listing them from the database. Careful not to delete something you want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['testing']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Project.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['testing']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this to completely remove the tutorial project from the database.\n",
    "Project.delete('tutorial')\n",
    "Project.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you have trajectories or models saved in a pre-existing folder for a project named tutorial, they are not deleted. The new project will iterate through pre-existing trajectory names and overwrite the data as each next tutorial is run. The data must be manually moved if desired, or deleted. Only the MongoDB storage associated with the project has been affected by `Project.delete('tutorial')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project = Project('tutorial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['testing']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a handle for our project. First thing is to set it up to work on a resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Resource`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is a resource? \n",
    "\n",
    "A `Resource` specifies a shared filesystem that may have one or more clusteres attached to it. This can be your local machine, a regular cluster, or even a group of cluster that can access the same FS (like Titan, Eos and Rhea do).\n",
    "\n",
    "Once you have chosen your place to store results, it is set for the project. The location should not be altered since all file references are made to match this resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use a local resource; your laptop or desktop machine for now. No cluster / HPC involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from adaptivemd import LocalResource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the Resource object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resource = LocalResource()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this object defines the path where all files will be placed, let's get the path to the shared folder. This path can be accessed from all workers on the resource. Using a local resource this is trivially the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$HOME/adaptivemd/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resource.shared_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, files will be placed in `$HOME/adaptivemd/`. You can change this using an option when creating the `Resource` \n",
    "\n",
    "```python\n",
    "LocalCluster(shared_path='$HOME/my/adaptive/folder/')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in more information about `Resource` setup consult the documentation about `Resource`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we save our configured `Resource` and initialize our empty project with it. This is done once for a project, a project's resource should not be altered. When this command is executed, the project is entered in the MongoDB under the name 'tutorial'. The directory for the project is not yet created, as workers manage the files and folders associated with a project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project.initialize(resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `File` Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from adaptivemd import File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a `File` object. Instead of just a string, these are used to represent files anywhere, on the cluster or your local application. There are some subclasses or _extensions_ of `File` that have additional meta information like `Trajectory` or `Frame`. The underlying base object of a `File` is called a `Location`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a first PDB file that is located on this machine at a relative path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdb_file = File('file://../files/alanine/alanine.pdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`File`, like any complex object in adaptivemd, can have a `.name` attribute that makes them easier to find later. You can either set the `.name` property after creation, or use a little helper method `.named()` to get a one-liner. This function will set `.name` and return itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about the possibilities to specify filelocation consult the documentation for `File`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdb_file.name = 'initial_pdb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.load()` at the end is important. It causes the `File` object to load the content of the file, and if you save the `File` object in the database, the actual file is stored with it. This way it can simply be rewritten on the cluster or anywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alanine.pdb'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdb_file.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Generator` Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TaskGenerators are instances whose purpose is to create tasks for execution. This is similar to the\n",
    "way Kernels work. A TaskGenerator will generate `Task` objects for you which will be translated into a `TaskDescription` and executed. In simple terms:\n",
    "\n",
    "**The task generator creates the bash scripts for you that run a simulation or run pyemma.**\n",
    "\n",
    "A task generator will be initialized with all parameters needed to make it work, and it will know what files need to be staged for the task to be used. `adaptivemd` relies primarily on two types of generators we will call:\n",
    "\n",
    "1. engine:   for producing simulation data\n",
    "2. modeller: for analyzing simulation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `Engine` is used to run trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from adaptivemd.engine.openmm import OpenMMEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A task generator will create tasks that workers use to run simulations. Currently, this means a little python script is created that will excute OpenMM. It requires conda to be added to the PATH variable, or at least openmm to be included in the python installation used by the resource. If you set up your resource correctly, then the task should execute automatically via a worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's do an example for the OpenMM engine. A small python script is created that makes OpenMM look like a executable. It runs a simulation by providing an initial frame, OpenMM specific system.xml and integrator.xml files, and some additional parameters like the platform name, how often to store simulation frames, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine = OpenMMEngine(\n",
    "    pdb_file=pdb_file,\n",
    "    system_file=File('file://../files/alanine/system.xml').load(),\n",
    "    integrator_file=File('file://../files/alanine/integrator.xml').load(),\n",
    "    args='-r --report-interval 1 -p CPU'\n",
    ").named('openmm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now an OpenMMEngine which uses the previously made pdb `File` object in the location defined by its `shared_path`. The same for the OpenMM XML files, along with some args to run using the `CPU` kernel, etc.\n",
    "\n",
    "Last we name the engine `openmm` to find it later, when we reopen the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'openmm'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to set the output types we want the engine to generate. We chose a stride of 10 for the `master` trajectory without selection, and save a second trajectory selecting only protein atoms and native stride.\n",
    "\n",
    "Note that the stride and frame number ALWAYS refer to the native steps used in the engine. In our example the engine uses `2fs` time steps. So master stores every `20fs` and protein every `2fs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine.add_output_type('master', 'master.dcd', stride=10)\n",
    "engine.add_output_type('protein', 'protein.dcd', stride=1, selection='protein')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selection must be an `mdtraj` formatted atom selection string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `PyEMMAAnalysis` modeller "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from adaptivemd.analysis.pyemma import PyEMMAAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object that computes an MSM model from existing trajectories that you pass it. It is initialized with a `.pdb` file that is used to create features between the $c_\\alpha$ atoms. This implementaton requires a PDB but in general this is not necessay. It is specific to my PyEMMAAnalysis show case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeller = PyEMMAAnalysis(\n",
    "    engine=engine,\n",
    "    outtype='protein',\n",
    "    features={'add_inverse_distances': {'select_Backbone': None}}\n",
    ").named('pyemma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we name it `pyemma` for later reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specified which output type from the engine we want to analyse. We chose the protein trajectories since these are faster to load and have better time resolution.\n",
    "\n",
    "The features dict expresses which features to use in the analysis. In this case we will use all inverse distances between backbone c_alpha atoms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add generators to project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to add the generators to the project for later usage. We pick the `.generators` store and just add it. Consider a store that works like a `set()` in python, where additionally when an object is added it is stored in the database. It contains objects only once and is not ordered. Therefore we need a name to find the objects later. Of course you can always iterate over all objects, but the order is not given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.generators.add(engine)\n",
    "project.generators.add(modeller)\n",
    "len(project.generators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that you cannot add the same engine twice. If you create a new but equivalent engine, it will be considered different and hence you can store it again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.generators.add(engine)\n",
    "len(project.generators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one initial trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are ready to run a first trajectory that we will store as a point of reference in the project. Also it is nice to see how it works in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a _Worker_ approach. This means simply that someone (in our case the user from inside a script or a notebook) creates a list of tasks to be done and some other instance (the worker) will actually do the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a `Trajectory` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create the parameters for the engine to run the simulation. We use a `Trajectory` object (a special `File` with initial frame and length) as the input. You could of course pass these things separately, but this way, we can actually reference the no yet existing trajectory and do stuff with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Trajectory should have a unique name, and so there is a project function to do this automatically. It uses numbers and makes sure that this number has not been used yet in the project. The data will be stored in the \"$HOME/adaptivemd/projects/`project`.`name`/trajs/`traj`.`name`\" directory, ie:\n",
    "\n",
    "> $HOME/adaptivemd/project/tutorial/trajs/00000000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/projects/tutorial/trajs/00000000/'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory = project.new_trajectory(engine['pdb_file'], 100, engine)\n",
    "#trajectory = project.new_trajectory(pdb_file, 100, engine)\n",
    "trajectory.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This says, initial is `alanine.pdb` run for 100 frames and is named `xxxxxxxx.dcd`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we need a trajectory object?\n",
    "\n",
    "You might wonder why a `Trajectory` object is necessary. You could just build a function that will take these parameters and run a simulation. At the end it will return the trajectory object. The same object we created just now.\n",
    "\n",
    "The main reason is to familiarize you with the general concept of asyncronous execution and so-called _Promises_. The trajectory object we built is similar to a _Promise_ so what is that exactly?\n",
    "\n",
    "A _Promise_ is a value (or an object) that represents the result of a function at some point in the future. In our case it represents a trajectory at some point in the future. Normal promises have specific functions do deal with the unknown result, for us this is a little different but the general concept stands. We create an object that represents the specifications of a `Trajectory` and so, regardless of the existence, we can use the trajectory as if it would exists:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(trajectory.length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and since the length is fixed, we know how many frames there are and can access them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame(sandbox:///{}/00000000/[20])\n"
     ]
    }
   ],
   "source": [
    "print(trajectory[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ask for a way to extend the trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<adaptivemd.engine.engine.TrajectoryExtensionTask object at 0x7fc4027d16a0>\n"
     ]
    }
   ],
   "source": [
    "print(trajectory.extend(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ask for a way to run the trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<adaptivemd.engine.engine.TrajectoryGenerationTask object at 0x7fc4027d19b0>\n"
     ]
    }
   ],
   "source": [
    "print(trajectory.run())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask to extend it, we can save it. We can reference specific frames in it before running a simulation. You could even build a whole set of related simulations this way without running a single frame. You might understand that this is pretty powerful especially in the context of running asynchronous simulations.\n",
    "\n",
    "Last, we did not answer why we have two separate steps: Create the trajectory first and then a task from it. The main reason is educational:\n",
    "> **It needs to be clear that a `Trajectory` object _can exist_ before running some engine or creating a task for it. The `Trajectory` _is not_ a result of a simulation action.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Create a `Task` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want that this trajectory actually exists so we have to make it. This requires a `Task` object that _knows_ to describe a simulation. Since `Task` objects are very flexible and can be complex, there are helper functions (i.e. factories) to create these in an easy manner like the ones we created before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trajectory (which knows its engine) to call `.run()` and save a reference to the task to work if you want to directly work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# equivalent to\n",
    "#task=engine.run(trajectory)\n",
    "task = trajectory.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, just take a trajectory description and turn it into a task that contains the shell commands and needed files, etc. Use the property `trajectory.exists` so see whether the trajectory object is associated with any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory.exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the task to the queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to add this task to the things we want to be done. This is easy and only requires saving the task to the project. This is done to the `project.tasks` bundle and once it has been stored it can be picked up by any worker to execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project.queue(trajectory)  # shortcut for project.tasks.add(task)\n",
    "#project.queue(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all we can do from here. To execute the tasks you need to run a worker using the `adaptivemdworker` command:\n",
    "\n",
    "```bash\n",
    "adaptivemdworker tutorial -l --verbose\n",
    "```\n",
    "\n",
    "For the simple setup in this tutorial, you can just navigate to the directory as follows (or add the directory to your PATH).\n",
    "\n",
    "```bash\n",
    "cd home/of/adaptivemd/scripts/\n",
    "```\n",
    "\n",
    "The worker is responsible for managing the project filestructure, so when both the worker is running and `project` entries and structure are changed in the database, the directory and subdirectories of \"adaptivemd/project/tutorial\" are created and modified. The folders \"trajs\" and \"models\" are populated with trajectories and models of the corresponding names as workers complete tasks that are entered into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fail'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now there are data files & folders associated with the trajectory\n",
    "project.wait_until(task.is_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are done for now, its also good practice to relieve your workers (and save yourself some compute time charges on HPC resources!). You don't have to, even if you're closing the project's database connection. They are associated with the project and will accept tasks at any point that are entered in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the 'one' method inherited from bundle to see available methods\n",
    "# for the worker type, such as 'execute'\n",
    "#project.workers.one.execute('shutdown')\n",
    "\n",
    "# but use 'all' method in practice to apply across all members of\n",
    "# the workers bundle in the typical case, where you have many workers\n",
    "project.workers.all.execute('shutdown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final `project.close()` will close the DB connection. The daemon outside the notebook would be closed separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
